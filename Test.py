from nltk import word_tokenize

text = input("Enter a sample text to be tokenized: ")
tokens = word_tokenize(text)
print(tokens)

